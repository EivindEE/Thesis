% Chapter Template

\chapter{Theory} % Main chapter title

\label{Theory} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref{Theory}. \emph{Theory}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Ontology and folksonomy}
One of the central concepts in semantic web technology is that of the ontology. 
In philosophy Ontology is the branch dealing with the study of which things 'exists', and if it is possible to categorize these things. 
For artificial intelligence \citet{Gruber1993} explained it as "an explicit specification of a conceptualization". 
That is than one commits to a given conceptualization of the domain in question, and formalize how we describe and reason about these conceptualizations. 
\citet{Pretorius2004} also gives a good overview of the history of the term, and show several of the interpretations and formalisms. 
One can also go to \citet{Noy1997} to find comparisons of several of the early ontologies, including WordNet which will be important in this thesis.

\citet{Shirky2007} criticizes the use of ontologies as a way of trying to enforce a structure on something that is by nature unstructured. 
He instead pushes the idea of common tagging. 
Part of the reason he criticizes the ontology approach is that it seem improbable that experts can know the needs of all the users a priori, and therefor that every ontology will prove to be inadequate.
%\citet{Doan2002} Tror ikke denne skal brukes
On the semantic web, ontologies are represented using collection of RDF( Resource Description Framework) triplets. 
These triplets are in the form of <subject, predicate, object>, much like simple declarative sentences \citep{Berners-Lee2001}. 
Each subject and predicate, and some objects, is represented by an URI( Universal Resource Identifier), which links to the resource that describes it.

While ontologies are formally constructed taxonomies, folksonomies are informal taxonomies generated by collecting tags or annotations from collaborative tagging systems on a given platform\citep{Tang2009}. 
\citet{Mika2005} has given a more formal definition of folksonomy where he sees a folksonomy as a set of tags T, 
$T \subseteq A \times C \times I$, where A is the set of users tagging, C is the set of tags, and I is the set of objects being tagged.
\citet{Gruber2007} suggested that one should add the source of the tag, and some kind of rating system to help filter out junk tags. 
\citet{Scerri2008} on the other hand suggests removing the objects being tagged from the ontology, 
seeing that the objects that are being described are not part of the tool to describe them.
Instead they like Gruber want to add the source of the tag, the number or times a tag occur, and the tagging behavior of each user.
\citet{Bang2008} explains the difference between ontologies and folksonomies by classifying them as a priori and a posteriori annotations. 
That is, ontologies are created by experts as ways on conceptualizing a domain, folksonomies on the other hand are samples of how people speak or think about a domain.
Folksonomies grew as a subject of research as it became popular for users to tag content on the internet with keywords they felt were relevant.

For users tags are convenient, since Adding additional tags can make it easier for humans to search and browse collections. This is especially true for multimedia content, which we don't yet have good tools for searching in \citep{Weinberger2008}.
Tags provide meta data about content in a way that makes sense to humans. From an information retrieval perspective this is interesting since it means that humans in some way add meaning to the content.
There however are several problems with using tags as the basis of a semantic web. \citet{Tang2009} mention several. 
Tags are supposed to be written in natural language, and natural language has words that are synonymes(words that are written in different ways, 
but mean the same), homonymes(words with different meanings that are written in the same way), or polynyms( a word that can have several meanings) 
making it unsuitable for computer reasoning since they are ambiguous \citep{Passant2008}. 
As \citet{Golder2005} mentions, users also operate on different levels of abstraction, which can make it harder to find interesting resources.
In addition to this comes the problem of non dictionary words, both new, or compound words, or simply words that have been misspelled\citep{Tonkin2006}.


There has been done a lot of research into how one can lift semantic data out of these unstructured tags.
\citet{Golder2005} has done research into the statistical analysis of tags. 
The analysis done here show that there seems to develop vocabularies of frequently used tags. 
This might help diminish the effect of misspelled and nonsense words. Similar findings were also reported by \citep{Shirky2007}

There has also been done research into automatic clustering. 
\citet{Mika2005} created clusters by creating weighted graphs, and compared using tag concurrence and actor interest as weights.  
\citet{Brooks2006} has done work an categorizing blogs entries by tags, to see if concurrence of tags indicated similar content. 
Using the most common tags did give some results, but only broad categories. The results were not better than extracting words that were given asserted to be relevant for the category.

\citep{Tang2009} tries to go further that clustering tags, and tries to build an hierarchical model from a folksonomy. 
They use a probabilistic model that takes into account the frequency and concurrence of tags and tries to generalize it to an ontology. 
The method does get good results in creating the hierarchy, but does also show som inappropriate sub/super category inferences.

\citet{Weinberger2008} suggests a method for removing ambiguity from tags,
 by suggesting additional tags to the user when the tag entered can belong to one of several distinct sets 

While there are many difficulties attached to merging the social and semantic web, and with lifting semantic data from tags, there are many researchers who stress the need for this \citep{Passant2007,Mika2005, Gruber2007}.

\section{WordNet and lexitags}
\label{TheoryWordNet}
Lexitags \citep{Veres2011} utilizes a different approach for getting semantic meaning out of tags that the approaches mentioned until now. 
Instead of analyzing existing folksonomies and try to lift semantic data out of these tags, the idea presented is to turn it around and make users attach meaning to the tags at input time.
This is done by letting users disambiguate the tags by using WordNet synsets, an idea that was also mentioned by.

WordNet is a lexical reference system that stores words in sets of synonymes called synsets. The idea is to separate the word form from the word sense. 
The underlying assumption is that the user already knows English, is familiar with the concepts that are conveyed, 
and doesn't need definitions to understand, but can use synonymes to identify the meaning they want to convey\citep{Miller1990}.

In addition to storing these synsets WordNet also contains information about the semantic relationship between different concepts. 
The synonymy relationship is obviously contained within each synset, 
though it should be mentioned that the definition used in WordNet is not one where substitution never changes the truth value of a sentence. 
WordNet uses a weaker definition where two word forms can be seen as synonymes in relation to some semantic context. 
The antonymy relationship is another relationship between word forms. While the exact definition of antonymy is hard to pin down, the intuitive notion that an antonym to x is not-x will take us a long way\citep{Miller1990}.

WordNet also stores information about hyponymy and hypernymy, which is a relationship between concepts.
 A hypernym can be explained as a generalization of some concept, a hyponym on the other hand can be seen as a specialization of a concept. 
 \{tree\} can for example be seen as a hyponym of \{plant\}, and the reverse relation is a hypernymy relation \citep{Veres2010}.

The fact that WordNet separates sense and form is good for our purposes, as we are interested in the sense, not the form of the word. 
Mapping tags to synsets removes the ambiguity that arrises from multiple spellings. 
At the same time, since the mapping preserves the form of the tag this can still be kept for analysis if one finds that there are significant differences in how different forms of a synset is used\citep{Veres2011}.
By enforcing this mapping to WordNet lexitags also gets access to the hierarchical knowledge therein, and can create lightweight ontologies by using hypernyms of the tags as SuperTags, a method introduced by \citet{Veres2010}.
The mapping to WordNet also add some perks. There is a mapping between WordNet and Schema.org\footnote{\url{https://github.com/mhausenblas/schema-org-rdf}}, and between WordNet and the SUMO( Suggested Upper Merged Ontology)\citep{Niles2003}.

Using WordNet to ground the semantics of the tags was idea also suggested by \citet{Cattuto2008}. But \citet{Cattuto2008} suggested using a post hoc analysis of the tags in a social network, instead of enforcing the mapping though an interface.

One critique of WordNet comes from \citet{Mika2005} who points out that while WordNet can catch lexical sameness, it lacks cultural awareness. The example used was that of the tie between Noah and the ark.
This tie would be obvious for most humans, and would most lightly be caught through clustering tags, but would not be caught by WordNet.

\citet{Passant2008} has also suggested a system where taggs are disambiguated by the user at input time. A difference between the systems is that Passant and Laublet suggested using URIs to online resources for disambiguation.

\section{WordNet}
In a written dictionary the most efficient way to organize data is to list the words alphabetically, 
since the act of finding words is the most labor intensive part.
As long as the dictionary is tied to an analog form it is hard to structure the content otherwise as it would make
finding words to difficult. 
With the advent of computer dictionaries, the act of looking up words is no longer time consuming or difficult, 
and the posibility to experiment with new structures for the dictionary became possible.

There are several differences between ordinary dictionaries and WordNet. 
The motivation behind WordNet was to create a dictionary that categorized words by the concepts they represented.
To accomplish this the structure was based on earlier psycholexicologic research\citep{Miller1990}.
 
One of the ways this psycholexicological background comes to sight is in that WordNet separates words into four syntactical categories: nouns, verbs, adjectives and adverbs\citep{Miller1995}.
This was based in part on work done by \citet{Fillenbaum1965} which showed that participants would most frequently 
assosiate words with other words from the same syntactical category.

This way of categorizing the words is able to take advantage of the fact that the different syntactical categories have different semantic structures.
In this thesis we will use the noun category of WordNet, and benefit from it's topoligical hierarchy.
For completeness we'll also mention that verbs are organized as entailment relations, 
while adjectives and adverbs are organized as N-dimentional hyperspaces\citep{Miller1990}.

The word 'word' is ambigious as it can be used to describe the representation of a word, and its underlying semantics.
Natural language is built on conventions that tie together utterances or symbols, with some thing or idea.
A symbol or utterance is the form of a word, while the thing or idea it represents is the words meaning.
We can represent this idea using Table \ref{table:LexicalMatrix}( see page \pageref{table:LexicalMatrix}).
The table shows a lexical matrix, which means to make explisit the relation between form $F$ and meaning $M$.
The forms and meanings are linked using entries $E_{(x,y)}$ which would state that meaning $M_x$ has the form $F_y$.
It also shows how both a single form can refere to several meanings, 
and how a single meaning can be represented using several forms.
Within the categories mentioned WordNet then tries to organize the words not by form, 
but by similarity of meaning.

This organisation is done by grouping words which are synonymes into sets. 
We will describe these sets of synonymes( synsets), by enclosing one or more word forms in curly brackets.
We could for example use the synset \{dog, domestic dog, Canis familiaris\} to describe the common dog. 
When precision is not the point we will use a short form like \{dog\} to describe the same meaning.
To organize words into synsets in this way ont first needs to have a clear idea about what one means by synonymy.
A strict definition of synonymy would claim that for form $F$ and $F'$ to be synonymous one must be able
to replace one with the other in any sentence without changing the truth value of that sentence.
This is similar to using Leibniz law of identity to qualify synonymy.
WordNet uses a weaker definition where one says that whether two forms are synonymes is dependent on the 
semantic context that they are inn, and that two words can be synonymes with respect to this semantic context\citep{Miller1990}.
This weaker definition still necessitates that words must be from the same syntactic category to be synonymous, 
and explains together with \citet{Fillenbaum1965} why it's useful to separate the syntactic categories.

\begin{table}[h]
	\centering
	\begin{tabular}{|c||ccccccc|}
		\hline
	        Word     & 	~		 & ~		 & Word Forms & ~ & ~ & ~ & ~		  \\ 
	        Meanings & F$_1$     & F$_2$     & F$_3$      & . & . & . & F$_n$     \\ \hline
	        M$_1$    & E$_{1,1}$ & E$_{1,2}$ & ~          & ~ & ~ & ~ & ~         \\ 
	        M$_2$    & ~         & E$_{2,2}$ & ~          & ~ & ~ & ~ & ~         \\ 
	        M$_3$    & ~         & ~         & E$_{3,3}$  & ~ & ~ & ~ & ~         \\ 
	        .        & ~         & ~         & ~          & . & ~ & ~ & ~         \\ 
	        .        & ~         & ~         & ~          & ~ & . & ~ & ~         \\ 
	        .        & ~         & ~         & ~          & ~ & ~ & . & ~         \\  
	        M$_m$    & ~         & ~         & ~          & ~ & ~ & ~ & E$_{m,n}$ \\
		\hline
	\end{tabular}
	\caption{A lexical matrix showing the relation between the forms and meanings of words, from \citet{Miller1990}}
	\label{table:LexicalMatrix}
\end{table}


\subsection{Hyponymy and hypernymy}
Hyponymy and hypernymy is the main way that nouns are organized in WordNet.
Hypo- and hypernymy are a different type of relation between words than synonymy.
While synonymy is a relation between different forms of a word, 
hypo- and hypernymy are relations between word meanings.
The two are the inverse relation of each other and we will explain them by defining what makes something a hypernym.
The concept $M$ is the hypernym of some other concept $M'$ if $M$ is such that native speakers of English would agree
with claims of the type "$M'$ is a type of $M$". 
As an example: \{animal\} would be a hypernym of \{dog\}, 
since a native user of English would agree that "a dog is a type of animal".
Hypernymy is transitive, meaning that if $M$ is a hypernym of $M'$, and $M'$ is a hypernym of $M''$, 
then $M$ is a hypernym of $M''$.
So since we know that dog is a type of animal, and that labrador is a type of dog, 
we also know that \{animal\} is a hypernym of \{labrador\}.
In addition to being transitive, hypernymy is also asymmetric.
This means that the fact that $M$ is a hypernym of $M'$ entails that $M'$ cannot be a hypernym of $M$.
When we know that \{animal\} is the hypernym of \{labrador\}, 
we also know that \{labrador\} is not a hypernym of \{animal\}\citep{Miller1990}.

Hyponymy is the inverse relation of hypernymy. 
This means that if $M$ is a hypernym of $M'$, then $M'$ is a hyponym of $M$. 
The fact that \{animal\} is a hypernym of \{labrador\} means that \{labrador\} is a hyponym of \{animal\}.
Hyponymy is also transitive and asymmetric\citep{Miller1990}.

Another aspect of this relation with respect to nouns is the fact that one asserts some type of inherence from the more
general hypernyms to its more specialized hyponyms. 
This means that if one knows that a concept has some properties, 
then the hyponymes of that concept will have all the same properties, 
in addition to the properties it has which distinguishes it from the concept\citep{Miller1990a}.
This has some basis in psycholexicologic research. 
\citet{Collins1969} showed that subjects used less time to say that a concept inhabited some property when that property
was viewed as more typical for concept, than if it were a property of some more general concept
( see Figure \ref{MemoryStructure}, page \pageref{MemoryStructure}).
A subject would for example use less time establishing that a shark is dangerous, a property associated with sharks;
than they would establishing that it had fins, a property associated with fish; or that it breaths, associated with animals( see Figure \ref{MemoryStructure}, page \pageref{MemoryStructure}).

Nouns in WordNet is organized in a hierarchal fashion. 
One organizational issue then is if one should organize the nouns as one, or several hierarchies.
WordNet started by organizing the nouns into 25 semantic prime categories.
When these had been created one realized that these contained natural groupings.
At this point one decided to add some top level synsets which tied these together, 
with the most general being \{entity\}\citep{Miller1990a}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.60\textwidth]{MemoryStructure.png}
        \caption{A hypothetical memory structure, from \protect \citet{Collins1969}}
        \label{MemoryStructure}
    \end{center}
\end{figure}
