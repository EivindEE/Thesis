% Chapter Template

\chapter{Theory} % Main chapter title
\label{Theory} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\lhead{Chapter \ref{Theory}. \emph{Theory}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

This chapter will give a review of some of the research done on user generated metadata and ontologies.
It will also introduce the core vocabularies and tool will be used in the development of the artefact,
and describe their origins and the state of research tied to them at the present.

\section{Ontology and folksonomy}
One of the central concepts in semantic web technology is that of the ontology.
In philosophy Ontology is the branch dealing with the study of which things 'exists', and if it is possible to categorize these things.
For artificial intelligence \citet{Gruber1993} explained it as "an explicit specification of a conceptualization".
That is, than one commits to a given conceptualization of the domain in question,
and formalize how one describes and reason about these conceptualizations.

\citet{Guarino1998} provided a description, which might be simpler, in which an ontology was described as a shared vocabulary
concerning some specific domain where the assumptions regarding each term in the vocabulary was made explicit.
\citet{Pretorius2004} talks about information sharing and reuse as one of the advantages of using ontologies to
capture conceptualizations.
Two tools that are committed to the same ontology can easily share their knowledge stores,
since they talk about the same things using the same explicit definitions of those things.
Reusing information also becomes easier since one can use existing knowledge stores by committing to the ontology they are
described with, or by finding mappings from that ontology to the one you want to use.

%One can also go to \citet{Noy1997} to find comparisons of several of the early ontologies, including WordNet which will be important in this thesis.

\citet{Shirky2007} criticizes the use of ontologies as a way of trying to enforce a structure on something that is by nature unstructured.
He instead pushes the idea of common tagging.
Part of the reason he criticizes the ontology approach is that it seem improbable that experts can know the needs of all the users a priori, and therefore that every ontology will prove to be inadequate.
%\citet{Doan2002} Tror ikke denne skal brukes

While ontologies are formally constructed taxonomies, folksonomies are informal taxonomies generated by collecting tags or annotations from collaborative tagging systems on a given platform\citep{Tang2009}.
\citet{Mika2005} has given a more formal definition of folksonomy where he sees a folksonomy as a set of tags T,
$T \subseteq A \times C \times I$, where A is the set of users tagging, C is the set of tags, and I is the set of objects being tagged.
\citet{Gruber2007} suggested that one should add the source of the tag, and some kind of rating system to help filter out junk tags.
\citet{Scerri2008} on the other hand suggests removing the objects being tagged from the ontology,
seeing that the objects that are being described are not part of the tool to describe them.
Instead they like Gruber want to add the source of the tag, the number or times a tag occur, and the tagging behavior of each user.
\citet{Bang2008} explains the difference between ontologies and folksonomies by classifying them as a priori and a posteriori annotations.
That is, ontologies are created by experts as ways on conceptualizing a domain, folksonomies on the other hand are samples of how people speak or think about a domain.
Folksonomies grew as a subject of research as it became popular for users to tag content on the internet with keywords they felt were relevant.

For users tags are convenient, since adding additional tags can make it easier for humans to search and browse collections.
This is especially true for multimedia content, which we don't yet have good tools for searching in \citep{Weinberger2008}.
Tags provide metadata about content in a way that makes sense to humans.
From an information retrieval perspective this is interesting since it means that humans in some way add meaning to the content.
There however are several problems with using tags as the basis of a semantic web.
\citet{Tang2009} mention several.
Tags are supposed to be written in natural language, and natural language has words that are synonyms (words that are written in different ways,
but mean the same), homonyms (words with different meanings that are written in the same way), or polynyms (a word that can have several meanings)
making it unsuitable for computer reasoning since they are ambiguous \citep{Passant2008}.
As \citet{Golder2005} mentions, users also operate on different levels of abstraction, which can make it harder to find interesting resources.
In addition to this comes the problem of non-dictionary words, both new, or compound words, or simply words that have been misspelled\citep{Tonkin2006}.


There has been done a lot of research into how one can lift semantic data out of these unstructured tags.
\citet{Golder2005} has done research into the statistical analysis of tags.
The analysis done here show that there seems to develop vocabularies of frequently used tags.
This might help diminish the effect of misspelled and nonsense words. Similar findings were also reported by \citep{Shirky2007}

There has also been done research into automatic clustering.
\citet{Mika2005} created clusters by creating weighted graphs, and compared using tag concurrence and actor interest as weights.
\citet{Brooks2006} has done work on categorizing blog entries by tags, to see if concurrence of tags indicated similar content.
Using the most common tags did give some results, but only broad categories. The results were not better than extracting words that were given asserted to be relevant for the category.

\citep{Tang2009} tries to go further that clustering tags, and tries to build an hierarchical model from a folksonomy.
They use a probabilistic model that takes into account the frequency and concurrence of tags and try to generalize it to an ontology.
The method does get good results in creating the hierarchy, but does also show some inappropriate sub/super category inferences.

\citet{Weinberger2008} suggests a method for removing ambiguity from tags,
 by suggesting additional tags to the user when the tag entered can belong to one of several distinct sets

While there are many difficulties attached to merging the social and semantic web,
and with lifting semantic data from tags, there are many researchers who stress the need for this \citep{Passant2007,Mika2005, Gruber2007}.
\citet{Veres2011} suggested one method one could use to merge these two approaches.
The idea presented involved disambiguating the tag at insertion time.
This idea and the implementation of a system that disambiguates tags will be discussed later in the chapter.

\subsection{RDF}
On the semantic web, ontologies are represented using collection of \nom{RDF}{Resource Description Framework} triples.
These triples are in the form of <subject, predicate, object>, much like simple declarative sentences \citep{Berners-Lee2001}.
Let's suppose we wanted to say that I know my supervisor Csaba.
An informal triple describing this could be
\begin{verbatim}
     <Eivind, knows, Csaba>
\end{verbatim}
"Eivind" is here the thing one talks about, the thing one wants to describe in some way.
The predicate "knows" is a relation the subject of the triple has to some other thing.
The object is the thing with which the subjects relates with respect to the predicate, in this case it is the thing that is known.
When representing triples using RDF one has to use a more stringent form.
The reason one describes things in RDF form is to give computers some way of reasoning about them,
so there must be some way to separate this "Eivind" from the other "Eivind"s out there,
there are after all alot of people named Eivind.
Disambiguation is achieved by using \nom{IRI}{Internationalized Resource Identifier}s to represent the resources one wants to talk about.
Using IRIs makes it possible to separate the thing one talks about from all the other similarly named things,
since each should uniquely identify a single thing.
The IRIs will also often be \nom{URL}{Uniform Resource Locator}s
that point to a location that describe the resource one talks about.
But the only requirement for choosing an IRI is that it is a unique identifier.

An example using IRIs to describe each resource could be as such:
\begin{verbatim}
     <
       "http://eivind.elseth.me",
       "http://xmlns.com/foaf/0.1/knows",
       "http://csabaveres.net"
     >
\end{verbatim}
Here the first IRI identifies whom we mean by "Eivind", namely "http://eivind.elseth.me".
The second IRI identifies what we mean by knows, and the third who it is that Eivind knows.
A machine parsing this can dereference the URLs to glean more information about the different resources.

The subject and predicate of a triple must always be resources represented by IRIs.
The objects can be either resources or literals, depending on what the predicate of the triple is.
A predicate describing something like "has name" would likely point to a string literal containing the name of the subject\citep{Hebeler2009Chp3}.

\section{WordNet}
\label{WordNet}
In a written dictionary the most efficient way to organize data is to list the words alphabetically,
since the act of finding words is the most labor intensive part.
As long as the dictionary is tied to an analog form it is hard to structure the content otherwise as it would make
finding words to difficult.
With the advent of computer dictionaries, the act of looking up words is no longer time consuming or difficult,
and the possibility to experiment with new structures for the dictionary became possible.

There are several differences between ordinary dictionaries and WordNet.
The motivation behind WordNet was to create a dictionary that categorized words by the concepts they represented.
To accomplish this the structure was based on earlier psycholexicologic research\citep{Miller1990}.

One of the ways this psycholexicological background comes to sight is in that WordNet separates words into four syntactical categories: nouns, verbs, adjectives and adverbs\citep{Miller1995}.
This was based in part on work done by \citet{Fillenbaum1965}, which showed that participants would most frequently
associate words with other words from the same syntactical category.

This way of categorizing the words is able to take advantage of the fact that the different syntactical categories have different semantic structures.
This thesis will use the noun category of WordNet, and benefit from it's topological hierarchy.

The word 'word' is ambiguous as it can be used to describe the representation of a word, and its underlying semantics.
Natural language is built on conventions that tie together utterances or symbols, with some thing or idea.
A symbol or utterance is the form of a word, while the thing or idea it represents is the words meaning.
This idea can be represent as in table \ref{table:LexicalMatrix}( see page \pageref{table:LexicalMatrix}).
The table shows a lexical matrix, which means to make explicit the relation between form $F$ and meaning $M$.
The forms and meanings are linked using entries $E_{(x,y)}$ that would state that meaning $M_x$ has the form $F_y$.
It also shows how both a single form can refer to several meanings,
and how a single meaning can be represented using several forms.
Within the categories mentioned WordNet then tries to organize the words not by form,
but by similarity of meaning.

\begin{table}[ht]
	\centering
	\begin{tabular}{|c||ccccccc|}
		\hline
	        Word     & 	~		 & ~		 & Word Forms & ~ & ~ & ~ & ~		  \\
	        Meanings & F$_1$     & F$_2$     & F$_3$      & . & . & . & F$_n$     \\ \hline
	        M$_1$    & E$_{1,1}$ & E$_{1,2}$ & ~          & ~ & ~ & ~ & ~         \\
	        M$_2$    & ~         & E$_{2,2}$ & ~          & ~ & ~ & ~ & ~         \\
	        M$_3$    & ~         & ~         & E$_{3,3}$  & ~ & ~ & ~ & ~         \\
	        .        & ~         & ~         & ~          & . & ~ & ~ & ~         \\
	        .        & ~         & ~         & ~          & ~ & . & ~ & ~         \\
	        .        & ~         & ~         & ~          & ~ & ~ & . & ~         \\
	        M$_m$    & ~         & ~         & ~          & ~ & ~ & ~ & E$_{m,n}$ \\
		\hline
	\end{tabular}
	\caption{A lexical matrix showing the relation between the forms and meanings of words, from \citet{Miller1990}}
	\label{table:LexicalMatrix}
\end{table}

This organization is done by grouping words that are synonyms into sets.
Figure \ref{Synset} shows these groupings for the word "dog".
These sets of synonyms (\nom{synset}{A set of words with similar meanings.}s)
car be described by enclosing one or more word forms in curly brackets.
The synset \{dog, domestic dog, Canis familiaris\} can be used to describe the common dog.
When precision is not the point a short form like \{dog\} can be used to describe the same meaning.
Later, when referring to a specific synset the form dog\#n\#1 will be used.
The first part here referes to the form of the word.
The second part is the grammatical group of the synset, in this case n for noun.
The third part is the numbering of the synset, used to separate it from the other meanings of the word "dog".

\fig{Synset}{The possible meanings of the word "dog"}

To organize words into synsets in this way one first needs to have a clear idea about what one means by synonymy.
A strict definition of synonymy would claim that for form $F$ and $F'$ to be synonymous one must be able
to replace one with the other in any sentence without changing the truth-value of that sentence.
This is similar to using Leibniz law of identity to qualify synonymy.
WordNet uses a weaker definition where one says that whether two forms are synonyms is dependent on the
semantic context that they are in, and that two words can be synonyms with respect to this semantic context\citep{Miller1990}.
This weaker definition still necessitates that words must be from the same syntactic category to be synonymous,
and explains together with \citet{Fillenbaum1965} why it's useful to separate the syntactic categories.

\subsection{Hyponymy and hypernymy}
\label{Hypernymy}
Hyponymy and hypernymy is the main way that nouns are organized in WordNet.
Hypo- and hypernymy are a different type of relation between words than synonymy.
While synonymy is a relation between different forms of a word,
hypo- and hypernymy are relations between word meanings.
The two are the inverse relation of each other and I will explain them by defining what makes something a hypernym.
The concept $M$ is the hypernym of some other concept $M'$ if $M$ is such that native speakers of English would agree
with claims of the type "$M'$ is a type of $M$".
As an example: \{animal\} would be a hypernym of \{dog\},
since a native user of English would agree that "a dog is a type of animal".
Hypernymy is transitive, meaning that if $M$ is a hypernym of $M'$, and $M'$ is a hypernym of $M''$,
then $M$ is a hypernym of $M''$.
So since we know that dog is a type of animal, and that Labrador is a type of dog,
we also know that \{animal\} is a hypernym of \{Labrador\}.
In addition to being transitive, hypernymy is also asymmetric.
This means that the fact that $M$ is a hypernym of $M'$ entails that $M'$ cannot be a hypernym of $M$.
When we know that \{animal\} is the hypernym of \{Labrador\},
we also know that \{Labrador\} is not a hypernym of \{animal\}\citep{Miller1990}.

Hyponymy is the inverse relation of hypernymy.
This means that if $M$ is a hypernym of $M'$, then $M'$ is a hyponym of $M$.
The fact that \{animal\} is a hypernym of \{Labrador\} means that \{Labrador\} is a hyponym of \{animal\}.
Hyponymy is also transitive and asymmetric\citep{Miller1990}.

Another aspect of this relation with respect to nouns is the fact that one asserts some type of inherence from the more
general hypernyms to its more specialized hyponyms.
This means that if one knows that a concept has some properties,
then the hyponyms of that concept will have all the same properties,
in addition to the properties it has which distinguishes it from the concept\citep{Miller1990a}.

This has some basis in psycholexicologic research.
\citet{Collins1969} showed that subjects used less time to say that a concept had some property when that property
was viewed as more prototypical for concept, than if it was seen a property of some more general concept.
Figure \ref{MemoryStructure} shows an example of such a hypothetical memory structure.
The tests performed showed that subjects would spend less time on deciding that a shark was dangerous than that it had fins or that it breaths.
This was in line with the hypothesis as being dangerous is more prototypical of a shark, while having fins or breathing
is something which is associated with more general concepts.

\fig{MemoryStructure}{{A hypothetical memory structure, adapted from \protect \citet{Collins1969}}}

Nouns in WordNet are organized in a hierarchal fashion.
One organizational issue then is if one should organize the nouns as one, or several hierarchies.
WordNet started by organizing the nouns into 25 semantic prime categories.
When these had been created one realized that these contained natural groupings.
At this point one decided to add some top-level synsets which tied these together,
with the most general being \{entity\}\citep{Miller1990a}.

\section{DBPedia}
DBPedia is an ontology based on data from Wikipedia.
Wikipedia is a successful open collaborative information project,
with about 4.2 million articles in English and 30 million all together
\footnote{As of 2013-04-16: \url{http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia}}.
The justification for the ontology is similar to the criticism mentioned by \citet{Shirky2007}
that it's difficult to know how a schema or ontology is going to be used before it's released on the web.
Instead of starting with an ontology then the work with DBPedia examine how one can extract unstructured data
from Wikipedia and transform it into a machine readable ontology.
The main contributions of the project has been\citep{Auer2007}:

\begin{itemize}
	\item Developing a framework for extracting information from Wikipedia to RDF
	\item Linking the items in the dataset to other ontologies
	\item Exposing the RDF dataset to the public
	\item Developing APIs for accessing the dataset
\end{itemize}

The extraction process is described in \citet{Auer2007a}.
DBPedia extracts information from templates on the Wikipedia pages.
The algorithm used looks through templates which have a high probability of containing structured information.
These templates are parsed looking for pairs of attributes and values.
The attributes correspond to predicates, and the values to objects of RDF statements.
The subject of the statements would be the topic of the page itself,
and it's given a unique URI in the DBPedia namespace by using it's Wikipedia URL.

There is also some post processing of the results to enhance the data further.
If the object of the statement is another Wikipedia resource,
the same process that exists for the subject is used to find the targets URL and link it to that.
In the cases where the object is a literal of a known type, then the data type information is added.

DBPedia is not a static ontology and in continuously being updated with data from Wikipedia,
making sure it's up to date with the newest terms and information.



\section{Lexitags}
\label{Lexitags}
\citet{Veres2011} marks one of the differences between the Web 2.0, or the social web and the semantic web as the
fact that the social web is based on giving users the ability to create free form, unstructured data or content.
The semantic web on the other hand is based on having a structure that allows for automatic machine processing of the content.
The advantage of the social web is that it provides a very low threshold for content providers,
but the lack of structure can provide long term difficulties since it can become hard to retrieve older content.
The semantic web on the other hand has a higher threshold,
but the structure makes retrieving the correct content easier,
since machines can reason about the content and be made responsible for finding the correct content.
The goal then would be to create a system that feels like a social system at input time,
but which creates structured markup that can be searched like a semantic system at retrieval time.

Analysis of tags by \citet{Tonkin2006} from delicious\footnote{\url{https://delicious.com}} and Flickr\footnote{\url{http://www.flickr.com}},
two websites that are focused on user generated content and tags,
showed that a large amount of the tags were "sloppy".
Sloppy tags could mean that the tags were misspelled,
that users had made compounded words my merging several words into one tag
or that they added non-alphanumeric characters.

The lexitags system is developed to work with tags and provides a solution to the problem of sloppy tags.
Lexitags solves the disambiguation problem by providing the user with a list of suggested senses for the tag provided.
These senses can be in the form of a synset or the description of the sense.
The user can then pick which sense that corresponds with the meaning the user wanted to convey\citep{Veres2011}.
The sense the user chooses will then be stored as a reference to one of the vocabularies that lexitags uses.
The vocabularies used by lexitags are WordNet and DBPedia.
Since WordNet mainly contains dictionary words,
DBPedia was chosen to complement it to cover things like names, places, or compound terms.

The lexitags server is used in \theartefact\ to find mappings from the natural language concepts to WordNet.
The artefact will treat the keywords that the user selects on the page as tags,
and use the lexitags interface to find disambiguated synsets describing the concepts they represent.

\section{Suggested upper merged ontology}
The suggested upper merged ontology(\nom{SUMO}{Suggested upper merged ontology}) was created based on work from the Standard Upper Ontology(\nom{SUO}{Standard Upper Ontology}),
and is one of the proposed candidates for SUO.
The work with SUO was started to fulfill the need for a large, free, public standard ontology
created by cross discipline consortium of contributors.

The idea behind a standard upper ontology is to simplify generating new knowledge and databases
by providing high level concepts that one can use to build upon,
and which can help with interoperability with other systems.
It could also ease the burden of working with legacy databases and knowledge stores,
as one can just map them to this common ontology, and use the language of the ontology to communicate.
It can also help combine different domain specific ontologies.
By having a common set of terms, even if just a subset,
it becomes easier to integrate the different ontologies\citep{Niles2001}.

The process involved in creating SUMO consisted of several steps.
To start with it was necessary to identify high lever ontologies which where not hampered by licensing issues.
As the name can imply, the basis of SUMO was to merge these ontologies together into one unified ontology,
using the upper concepts of these ontologies.
The upper levels are shown in figure \ref{SUMOUpper}( page \pageref{SUMOUpper}).
Physical describes all those things that have some position in time and space, things like atoms, dogs and galaxies.
The other main category, Abstract, covers those things that do not have some physical extension, like numbers,
justice, and song.
In addition to these types there is a unifying top-level type called entity, which encompasses all things physical and abstract.

Before the merge could start, the different ontologies were translated into the same knowledge representation format,
to make sure they conformed to the same syntax\citep{Niles2001}.
SUMO is written in the SUO knowledge interchange format,
but is also translated to the web ontology language(\nom{OWL}{Web Ontology Language})\citep{Benzmuller2012}.
Then the ontologies that contained the highest-level concepts were merged by unifying those concepts that were equal,
and positioning the other concepts in relation to them.
This upmost level was then used as a basis for aligning the other lower level ontologies\citep{Niles2001}.


Merging the lower level terms fell into four general cases.
In the simplest case the term didn't correspond to terms in the other ontologies,
but was useful and suitable, and didn't break with any of the philosophical decisions made.

Other terms were found to be unsuitable for SUMO.
Since SUMO was supposed to be a top-level ontology terms that were to narrow,
or which had little practical use were discarded.

In other cases a term would refer to the same meaning as some other term in the ontology.
In these cases where it was only the term chosen to convey the meaning that differed one can map both term to the same
term in the final ontology.
In the last case the terms has overlapping meaning.
In these cases one either kept both the terms, or picked which one to use in the merged ontology.

\fig{SUMOUpper}{{The upper levels of SUMO. Adapted from \protect \citet{Niles2001}}}

\section{Schema.org}
\label{schemadotorg}
Schema.org\footnote{\url{http://schema.org}} is a joint venture by Google, Yahoo, and Bing,
with the expressed goal of creating richer search results by getting content creators add metadata to their pages.
In addition to giving the search engines extra metadata about the content of the page,
the search engines also use them to provide information for rich snippets\citep{Guha2011}.
Rich snippets are small pieces of HTML which in some way can add information to a search result,
showing the number of stars a product got in a review, a picture of the author, part of the multimedia content
or some other enriched content\citep{Mayer2009}.
Figure \ref{RichSnippet} shows how the search result for the \theartefact\ homepage has been made more prominent by adding
an image of the author of the page.
\fig{RichSnippet}{The rich snippet displayed for the \theartefact\ homepage}
Schema.org is tied closely to the microdata format\footnote{\url{http://dev.w3.org/html5/md-LC/}},
but does at the moment still allow RDFa.

Schema.org is not a general or upper level, or a domain specific ontology,
but can be regarded as a middle level ontology.
That is to say that it does not try to be neither an all-encompassing ontology,
nor an ontology which covers everything within a domain.
The role as a midlevel ontology involves covering the most common use cases without having the entire hierarchy modeled.
The top levels of schema.org are shown in figure \ref{TopLevelSchemaOrg}.

\fig{TopLevelSchemaOrg}{The top-level entities of schema.org}

Inspecting the ontology shows that it is clearly geared toward search engine and commercial usage,
both in the concepts that are selected, and in the hierarchal structure of the ontology\citep{Ronallo2012}.
It is also a fairly small ontology with only 577 types, excluding the 10 data types.

If schema.org does not contain the term needed they suggest extending the scheme by adding sub-types or sub-properties.
The mechanism they provide for extending the vocabulary is string concatenation.
Schema.org defines person using the following URL:
\begin{verbatim}
	http://schema.org/Person
\end{verbatim}
If you need to say that this person is a minister you would then extend Person by adding Minister at the end:
\begin{verbatim}
	http://schema.org/Person/Minister
\end{verbatim}
The intention is to have the extension mechanism working in a way that will give search engines some information,
even when they come across unknown terms.
It is also intended as a way to let the ontology grow organically,
as terms which become common can be included in the schema.

This extension mechanism does however not conform to the microdata specification as \citet{Tennison2011} explains.
The specification says that URIs should be opaque identifiers,
and that one should not infer meaning through string parsing.
The mechanism also comes into difficulties when it comes to disambiguation.
It is impossible to infer from the URI itself if Person/Minister refers to a clergyman, or to a minister of state.
This problem would also persist when incorporating the term in the schema,
and might make markup that used to be correct say something that was unintended.
